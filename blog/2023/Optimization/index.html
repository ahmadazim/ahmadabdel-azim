<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Statistical Optimization Techniques | Ahmad  Abdel-Azim</title>
    <meta name="author" content="Ahmad  Abdel-Azim">
    <meta name="description" content="An overview and implementation of statistical methods used for optimization">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    
    <!-- Sidebar Table of Contents -->
    <link href="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.css" rel="stylesheet">
    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/webicon.png">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://ahmadabdel-azim.com/blog/2023/Optimization/">

    <!-- Dark Mode -->
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="https://ahmadabdel-azim.com"><span class="font-weight-bold">Ahmad Abdel-Azim</span>
          </a>
          <!-- Social Icons -->
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <!-- <li class="nav-item ">
                <a class="nav-link" href="/">home</a>
              </li> 
               -->
              
              <!-- Blog -->
              <!-- <li class="nav-item active">
                <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a>
              </li> -->

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">Publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">Projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/blog/">Blog</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">CV</a>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        
        <div class="row">
          <!-- main content area -->
          <div class="col-sm-11">
            <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Statistical Optimization Techniques</h1>
    <p class="post-meta">July 3, 2023</p>
    <p class="post-tags">
      <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>
        ·  
        <a href="/blog/tag/algorithms">
          <i class="fas fa-hashtag fa-sm"></i> algorithms</a>  
          <a href="/blog/tag/statistics">
          <i class="fas fa-hashtag fa-sm"></i> statistics</a>  
          <a href="/blog/tag/tutorials">
          <i class="fas fa-hashtag fa-sm"></i> tutorials</a>  
          

    </p>
  </header>

  <article class="post-content">
    
    <div id="markdown-content">
      <p>Central to solving many scientific and machine learning problems is optimization. Excluding a handful specifically-structured problems, we typically have no guarantee of finding global optima; instead, we settle for reasonable good local modal solutions. Thus, optimization has largely taken the form of mode finding. Further, note that typically, mode finding of some target function is equivalent to root finding for its gradient, if it exists.</p>

<p><br></p>

<h3 id="overflow-and-underflow">Overflow and Underflow</h3>

<p>Developing robust strategies to deal with underflow and overflow is often of critical importance to many optimization problems. For example, in the context of statistical inference, we may want to maximize the likelihood function to derive the maximum likelihood estimator (MLE); however, especially with larger samples sizes, directly computing the likelihood can often lead to overflow or underflow. We typically never want to multiple many probabilities or density values <em>directly</em>. Instead, we can operate on the logarithmic scale when possible. When computing the summation or product of many small or large numbers, it is better to do this properly on the logarithmic scale.</p>

<p>Take the following problem for an example of dealing with underflow in particular. Let us say we have a potentially biased coin whose probability of showing head is \(\theta\), and <em>a priori</em> we know \(\theta\) is either 0.5 or 0.6, with equal probabilities. We toss the coin \(n=10,000\) times and observe \(x=5,510\) heads. Our goal then is to compute the posterior probability \(\Pr(\theta = 0.6 \\| n, x)\).</p>

<p>We can first find a simplified expression for this posterior probability of interest using Bayes Theorem and LOTP (note, \(n\) is known, but we include it in the expression below for clarity),</p>

\[\begin{aligned}
\Pr(\theta = 0.6 | n, x) &amp;= \frac{\Pr(x|n, \theta = 0.6) \Pr(\theta = 0.6)}{\Pr(x | n)} \\&amp;=\frac{\frac12 {n\choose x}(0.6)^x (0.4)^{n-x}}{\frac12 {n \choose x}(0.6)^x (0.4)^{n-x} + \frac12 {n\choose x} (0.5)^x (0.5)^{n-x}} \\ &amp;= \frac{(0.6)^x (0.4)^{n-x}}{(0.6)^x (0.4)^{n-x} +  (0.5)^n }
\end{aligned}\]

<p>We can see from the expression above that we are in fact working with some very small probabilities; \(0.6^{5510}\) evaluates directly to 0 in R due to underflow. To avoid such underflow errors, we can instead work with log-posterior probabilities,</p>

\[\begin{aligned}
\log \Pr(\theta = 0.6 | n, x) &amp;= \log \left[(0.6)^x (0.4)^{n-x}\right] -\log \left[(0.6)^x (0.4)^{n-x} +  (0.5)^n \right] \\ &amp;= x\log 0.6 + (n-x)\log 0.4 \\ &amp;\;\quad- \left[a + \log\left(\exp\left[x\log 0.6 + (n-x)\log 0.4 - a\right]+\exp\left[n\log 0.5 - a\right] \right)\right]
\end{aligned}\]

<p>where \(a =\max\{\left(x\log 0.6 + (n-x)\log 0.4\right) , n\log0.5\}\). Note that above, we are implementing the LogSumExp function (also known as the “SoftMax trick”) to compute the log of the denominator (which is the log of summed exponentiated log terms). This is a simple idea which essentially involves subtracting the maximum, \(a\), from each exponentiated term, thereby reducing underflow errors.</p>

<p>With these robust strategies defined for dealing with underflow, we can now implement our ideas below to compute the posterior probabilities of interest.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">compute_posterior</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">return_val</span><span class="o">=</span><span class="nb">F</span><span class="p">,</span><span class="w"> </span><span class="n">verbose</span><span class="o">=</span><span class="nb">T</span><span class="p">){</span><span class="w">
  </span><span class="n">log_numer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="o">*</span><span class="nf">log</span><span class="p">(</span><span class="m">0.6</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="n">x</span><span class="p">)</span><span class="o">*</span><span class="nf">log</span><span class="p">(</span><span class="m">0.4</span><span class="p">)</span><span class="w">
  </span><span class="n">a</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">max</span><span class="p">(</span><span class="n">log_numer</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="o">*</span><span class="nf">log</span><span class="p">(</span><span class="m">0.5</span><span class="p">))</span><span class="w">
  </span><span class="n">log_denom</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nf">log</span><span class="p">(</span><span class="nf">exp</span><span class="p">(</span><span class="n">log_numer</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">a</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nf">exp</span><span class="p">(</span><span class="n">n</span><span class="o">*</span><span class="nf">log</span><span class="p">(</span><span class="m">0.5</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">a</span><span class="p">))</span><span class="w">
  </span><span class="n">log_posterior</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">log_numer</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">log_denom</span><span class="w">
  
  </span><span class="k">if</span><span class="p">(</span><span class="n">verbose</span><span class="p">){</span><span class="w">
    </span><span class="n">cat</span><span class="p">(</span><span class="n">paste0</span><span class="p">(</span><span class="s1">'log-posterior prob: '</span><span class="p">,</span><span class="w"> </span><span class="n">log_posterior</span><span class="p">,</span><span class="s1">'\n'</span><span class="p">))</span><span class="w">
    </span><span class="n">cat</span><span class="p">(</span><span class="n">paste0</span><span class="p">(</span><span class="s1">'    posterior prob: '</span><span class="p">,</span><span class="w"> </span><span class="nf">exp</span><span class="p">(</span><span class="n">log_posterior</span><span class="p">),</span><span class="s1">'\n'</span><span class="p">))</span><span class="w">
  </span><span class="p">}</span><span class="w">
  
  </span><span class="k">if</span><span class="p">(</span><span class="n">return_val</span><span class="p">)</span><span class="w"> </span><span class="nf">return</span><span class="p">(</span><span class="n">log_posterior</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">
</span><span class="n">compute_posterior</span><span class="p">(</span><span class="m">10000</span><span class="p">,</span><span class="w"> </span><span class="m">5510</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## log-posterior prob: -0.0664927009465828
##     posterior prob: 0.935669745334858
</code></pre></div></div>

<p>We find that the posterior probability that \(\theta = 0.6\) is roughly 0.94. For fun, we can evaluate how this posterior probability varies depending on the number of heads we observe out of \(n\) coin flip trials.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ns</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">25</span><span class="p">,</span><span class="w"> </span><span class="m">50</span><span class="p">,</span><span class="w"> </span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="m">1000</span><span class="p">,</span><span class="w"> </span><span class="m">10000</span><span class="p">)</span><span class="w">
</span><span class="n">head_prop</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">seq</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">0.001</span><span class="p">)</span><span class="w">
</span><span class="n">probs_df</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">rep</span><span class="p">(</span><span class="n">ns</span><span class="p">,</span><span class="w"> </span><span class="n">each</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">length</span><span class="p">(</span><span class="n">head_prop</span><span class="p">)),</span><span class="w"> 
                      </span><span class="n">heads</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">rep</span><span class="p">(</span><span class="n">head_prop</span><span class="p">,</span><span class="w"> </span><span class="nf">length</span><span class="p">(</span><span class="n">ns</span><span class="p">)),</span><span class="w"> 
                      </span><span class="n">log_post_probs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">)</span><span class="w">
</span><span class="n">probs_df</span><span class="o">$</span><span class="n">heads_obs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">probs_df</span><span class="o">$</span><span class="n">heads</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">probs_df</span><span class="o">$</span><span class="n">n</span><span class="w">
</span><span class="n">probs_df</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">probs_df</span><span class="p">[</span><span class="nf">round</span><span class="p">(</span><span class="n">probs_df</span><span class="o">$</span><span class="n">heads_obs</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">probs_df</span><span class="o">$</span><span class="n">heads_obs</span><span class="p">,]</span><span class="w">

</span><span class="k">for</span><span class="p">(</span><span class="n">n</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">ns</span><span class="p">){</span><span class="w">
  </span><span class="n">start_idx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">which</span><span class="p">(</span><span class="n">probs_df</span><span class="o">$</span><span class="n">n</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">n</span><span class="p">)[</span><span class="m">1</span><span class="p">]</span><span class="w">
  </span><span class="n">idxs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">start_idx</span><span class="o">:</span><span class="p">(</span><span class="n">start_idx</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="n">probs_df</span><span class="o">$</span><span class="n">n</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="m">-1</span><span class="p">)</span><span class="w">
  </span><span class="n">obs_heads</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">probs_df</span><span class="o">$</span><span class="n">heads_obs</span><span class="p">[</span><span class="n">idxs</span><span class="p">]</span><span class="w">
  </span><span class="n">lpost_probs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">lapply</span><span class="p">(</span><span class="n">obs_heads</span><span class="p">,</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> 
    </span><span class="n">compute_posterior</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">return_val</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">,</span><span class="w"> </span><span class="n">verbose</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">unlist</span><span class="w">
  </span><span class="n">probs_df</span><span class="o">$</span><span class="n">log_post_probs</span><span class="p">[</span><span class="n">idxs</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">lpost_probs</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="n">ggplot</span><span class="p">(</span><span class="n">probs_df</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">heads</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">exp</span><span class="p">(</span><span class="n">log_post_probs</span><span class="p">),</span><span class="w"> </span><span class="n">color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">as.factor</span><span class="p">(</span><span class="n">n</span><span class="p">)))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_line</span><span class="p">(</span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.8</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> 
  </span><span class="n">ylab</span><span class="p">(</span><span class="s1">'Posterior probability theta = 0.6'</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> 
  </span><span class="n">xlab</span><span class="p">(</span><span class="s1">'Proportion of heads observed'</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> 
  </span><span class="n">geom_vline</span><span class="p">(</span><span class="n">xintercept</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.55</span><span class="p">,</span><span class="w"> </span><span class="n">color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">'gray20'</span><span class="p">,</span><span class="w"> </span><span class="n">linetype</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">'dashed'</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> 
  </span><span class="n">guides</span><span class="p">(</span><span class="n">color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">guide_legend</span><span class="p">(</span><span class="s1">'Number of \nCoin Flips (n)'</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>
<p><img src="/assets/img/blogPosts/optim1.png" style="width:75%; display: block; margin: auto;">
<br></p>

<p>As expected, we see that as the proportion of heads increases, the posterior probability that \(\theta = 0.6\) increases, with an inflection point at \(\theta = 0.55\). Further, with larger sample sizes, the increase in posterior probability past \(0.5\) increases more sharply; at larger samples sizes, we are more sure in our inference for a given porportion of heads observed.</p>

<p>Such computations would not be possible without robust strategies for dealing with underflow. Now, back to our discussion of optimization.</p>

<p><br></p>

<h3 id="simulating-data-for-optimization">Simulating Data for Optimization</h3>

<p>Throughout our discussion of different optimization techniques, we can use the probit regression model as a motivating example. We can simulate data from this model and apply various optimization algorithms to this data for the sake of comparison.</p>

<p>The probit regression model assumes the following relationship between a binary response \(y_i\) and a \(p\)-dimensional covariate vector \(\mathbf x_i\):</p>

\[P(y_i = 1 | \mathbf x_i) = \Phi(\mathbf x_i^\top \boldsymbol \beta), \qquad i = 1,\dots,n\]

<p>where \(\boldsymbol \beta = (\beta_1, \dots, \beta_p)^\top\) is an unknown \(p\)-dimensional vector of coefficients, and \(\Phi\) is the CDF of the standard Normal. Suppose we observe the \((y_i,\mathbf x_i)\)’s and are interested in estimating \(\boldsymbol\beta\).</p>

<p>We can simulate \(\mathbf{x}_i'\overset{iid}\sim \mathcal{N}(0,2\mathbb{I}_p)\) for \(i = 1,\dots,n\) with \(n=300\). Note that to do this, we can simulate each individual component of \(\mathbf{x}_i\) independently. Then, we can construct a coefficients vector \(\boldsymbol{\beta}\) as follows: \(\beta_j = 1\) if \(j\) is odd and -1 otherwise, for \(j = 1,\dots,p\). We take this \(\boldsymbol\beta\) to be the “true” underlying coefficient vector that we try to estimate using a variety of optimization techniques. Finally, we can use \(\mathbf{x}_i\) and \(\boldsymbol{\beta}\) to simulate \(y_i\). We can simulate data for both \(p = 1\) (one-dimensional) and \(p = 10\) (multi-dimensional).</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">300</span><span class="w">
</span><span class="n">sim_probit</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">p</span><span class="p">){</span><span class="w">
    </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sapply</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="n">rnorm</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="n">mean</span><span class="o">=</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="n">sd</span><span class="o">=</span><span class="nf">sqrt</span><span class="p">(</span><span class="m">2</span><span class="p">)))</span><span class="w">
    </span><span class="n">beta</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">as.matrix</span><span class="p">(</span><span class="n">ifelse</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="n">p</span><span class="w"> </span><span class="o">%%</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="n">yes</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">-1</span><span class="p">,</span><span class="w"> </span><span class="n">no</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">))</span><span class="w">
    </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pnorm</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">beta</span><span class="p">)</span><span class="w">
    </span><span class="nf">return</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">beta</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">))</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="c1"># Try p=1 and p=10</span><span class="w">
</span><span class="n">set.seed</span><span class="p">(</span><span class="m">221</span><span class="p">)</span><span class="w">
</span><span class="n">sim_p1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sim_probit</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="m">1</span><span class="p">)</span><span class="w">
</span><span class="n">sim_p10</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sim_probit</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="m">10</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>With our data simulated, the goal is now to find the optimal underlying \(\boldsymbol\beta\) that generated our simulated data (which we pretend that we do not know). We can take a likelihood-based approach to finding the optimal underlying \(\boldsymbol\beta\).</p>

<p><br></p>

<h4 id="evaluating-the-log-likelihood">Evaluating the Log-Likelihood</h4>

<p>First, we will need to write a robust subroutine to evaluate the log-likelihood function of \(\boldsymbol\beta\), which can be simplified by taking advantage of some useful properties of logarithms,</p>

\[\begin{aligned}
\ell(\boldsymbol{\beta}) &amp;= \log\prod_{i=1}^n \left(\Phi(\mathbf{x}_i^\top\boldsymbol{\beta})\right)^{y_i} \left(1 - \Phi(\mathbf{x}_i^\top\boldsymbol{\beta})\right)^{1-y_i} \\ &amp;= \sum_{i=1}^n y_i \log\left(\Phi(\mathbf{x}_i^\top\boldsymbol{\beta})\right) + (1-y_i)\log\left(1 - \Phi(\mathbf{x}_i^\top\boldsymbol{\beta})\right) \\ &amp;= \sum_{i=1}^n y_i \log\left(\Phi(\mathbf{x}_i^\top\boldsymbol{\beta})\right) + (1-y_i)\log\left( \Phi(-\mathbf{x}_i^\top\boldsymbol{\beta})\right)
\end{aligned}\]

<p>Note that in the log-likelihood simplification above, we evaluate the term \(\left(1 - \Phi(\mathbf{x}_i^\top\boldsymbol{\beta})\right)\) as \(\Phi(-\mathbf{x}_i^\top\boldsymbol{\beta})\) by the symmetry of the Normal CDF. This is advantageous here as it avoid numerical imprecision errors if \(\Phi(\mathbf{x}_i^\top\boldsymbol{\beta})\) returns a value too close to 1 (specifically greater than 0.9999999 in R); in such cases R would simply evaluate \(\left(1 - \Phi(\mathbf{x}_i^\top\boldsymbol{\beta})\right)\) as 0, which will return \(-\text{Inf}\) when we subsequently take the logarithm.</p>

<p>We can now implement this into a subroutine to evaluate the log-likelihood below.</p>
<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">eval_logLik</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">beta</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="n">cdf_vals</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pnorm</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">beta</span><span class="p">)</span><span class="w">
  </span><span class="n">ncdf_vals</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pnorm</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">beta</span><span class="p">)</span><span class="w">
  </span><span class="n">loglik_terms</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">y</span><span class="o">*</span><span class="nf">log</span><span class="p">(</span><span class="n">cdf_vals</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">(</span><span class="m">1</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="nf">log</span><span class="p">(</span><span class="n">ncdf_vals</span><span class="p">)</span><span class="w">
  </span><span class="nf">return</span><span class="p">(</span><span class="nf">sum</span><span class="p">(</span><span class="n">loglik_terms</span><span class="p">))</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p><br></p>

<h4 id="evaluating-the-gradient-of-the-log-likelihood">Evaluating the Gradient of the Log-Likelihood</h4>

<p>As mentioned in the Introduction earlier, typically, mode finding of some target function (the log-likelihood in this case) is equivalent to root finding for its gradient. We can thus write a subroutine to evaluate the gradient of the log-likelihood function. This will prove useful for some optimization techniques we discuss next. First, we solve for this gradient mathematically.</p>

\[\begin{aligned}
\nabla_{\boldsymbol{\beta}}\ell(\boldsymbol{\beta}) &amp;= \nabla_{\boldsymbol{\beta}} \left[\sum_{i=1}^n y_i \log\left(\Phi(\mathbf{x}_i^\top\boldsymbol{\beta})\right) + (1-y_i)\log\left( \Phi(-\mathbf{x}_i^\top\boldsymbol{\beta})\right)\right] \\ &amp;= \sum_{i=1}^n \left[\nabla_{\boldsymbol{\beta}} y_i \log\left(\Phi(\mathbf{x}_i^\top\boldsymbol{\beta})\right)+ \nabla_{\boldsymbol{\beta}}(1-y_i)\log\left( \Phi(-\mathbf{x}_i^\top\boldsymbol{\beta})\right)\right] \\ &amp;= \sum_{i=1}^n \left[\frac{y_i}{\Phi(\mathbf{x}_i^\top\boldsymbol{\beta})} \varphi(\mathbf{x}_i^\top\boldsymbol{\beta}) \mathbf{x}_i + \frac{y_i - 1}{\Phi(-\mathbf{x}_i^\top\boldsymbol{\beta})} \varphi(-\mathbf{x}_i^\top\boldsymbol{\beta}) \mathbf{x}_i \right] \\ &amp;= \sum_{i=1}^n \left[ \left( y_i\frac{\varphi(\mathbf{x}_i^\top\boldsymbol{\beta})}{\Phi(\mathbf{x}_i^\top\boldsymbol{\beta})} + (y_i - 1)\frac{\varphi(\mathbf{x}_i^\top\boldsymbol{\beta})}{\Phi(-\mathbf{x}_i^\top\boldsymbol{\beta})} \right)\mathbf{x}_i \right]
\end{aligned}\]

<p>We can now implement this into a subroutine below.</p>
<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">eval_grad_logLik</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">beta</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="n">cdf_vals</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pnorm</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">beta</span><span class="p">)</span><span class="w">
  </span><span class="n">ncdf_vals</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pnorm</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">beta</span><span class="p">)</span><span class="w">
  </span><span class="n">pdf_vals</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dnorm</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">beta</span><span class="p">)</span><span class="w">
  
  </span><span class="n">scalars</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">y</span><span class="o">*</span><span class="n">pdf_vals</span><span class="o">/</span><span class="n">cdf_vals</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">(</span><span class="n">y</span><span class="m">-1</span><span class="p">)</span><span class="o">*</span><span class="n">pdf_vals</span><span class="o">/</span><span class="p">(</span><span class="n">ncdf_vals</span><span class="p">)</span><span class="w">
  </span><span class="n">gradient</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">t</span><span class="p">(</span><span class="n">scalars</span><span class="p">)</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">x</span><span class="p">)</span><span class="w">
  
  </span><span class="nf">return</span><span class="p">(</span><span class="n">gradient</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>With all of this defined, we can finally discuss the first optimization, or more specifically root finding, method: Bisection.</p>

<p><br></p>

<h3 id="bisection-method">Bisection Method</h3>

<p>Perhaps the simplest of all root finding methods for a 1-dimensional continuous function \(f(x)\) is the bisection method. The algorithm proceeds as follows:</p>
<ul>
  <li>Initialize by finding two numbers \(a &lt; b\) such that \(f(a) \cdot f(b) &lt; 0\), and set \(\ell = a\) and \(u = a\).</li>
  <li>Then, bisect by letting \(c = (\ell + u)/2\) and computing \(f(c)\). If it is sufficiently close to 0, terminate. Otherwise set \(u \leftarrow c\) if \(f(\ell) \cdot f(c) &lt; 0\) and \(\ell \leftarrow c\) if \(f(\ell) \cdot f(c) &gt; 0\).</li>
</ul>

<p>This algorithm converges fairly quickly, as the “error”, or distance from the root, is essentially cut in half with each iteration. Again, if we seek to find optima for a differentiable function \(g(x)\), we can find the root of its derivative \(g′(x)\), requiring only knowledge of the initialization parameters \(a,b\) that bound a local optima.</p>

<p>Returning to out probit regression model, for \(p=1\), we can write a bisection algorithm to find the MLE \(\hat\beta_{MLE}\). We seek to find a root of the gradient of the log-likelihood, which is equivalent to finding the maximum of the log-likelihood since the log-likelihood of a probit regression is globally concave.</p>

<p>Let \(f(x)\) denote the gradient of the log-likelihood evaluated at \(\beta = x\). Our implementation below first takes in two numbers \(a &lt; b\) such that \(f(a)f(b) &lt; 0\) (otherwise an error is returned). The bisection algorithm then proceeds as described earlier. A plotting parameter is also included to visualize the convergence of the bisection algorithm.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">bisection_find</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">fx</span><span class="p">,</span><span class="w"> </span><span class="n">epsilon</span><span class="p">,</span><span class="w"> </span><span class="n">initials</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">plotting</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="n">l</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">initials</span><span class="p">);</span><span class="w"> </span><span class="n">u</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">max</span><span class="p">(</span><span class="n">initials</span><span class="p">)</span><span class="w">
  </span><span class="k">if</span><span class="p">(</span><span class="n">fx</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">beta</span><span class="o">=</span><span class="n">l</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">fx</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">beta</span><span class="o">=</span><span class="n">u</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">)</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="m">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nf">return</span><span class="p">(</span><span class="s2">"ERROR: Initial values provided do not evaluate to have opposite signs. 
           Select different values.\n"</span><span class="p">)</span><span class="w">
  </span><span class="p">}</span><span class="w">
  
  </span><span class="c1"># First iteration:</span><span class="w">
  </span><span class="n">iter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="w">
  </span><span class="n">cx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">l</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">u</span><span class="p">)</span><span class="o">/</span><span class="m">2</span><span class="w">
  </span><span class="n">fc</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">fx</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">beta</span><span class="o">=</span><span class="n">cx</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">)</span><span class="w">
  </span><span class="n">c_rec</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="n">cx</span><span class="p">)</span><span class="w">
  </span><span class="n">fc_rec</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="n">fc</span><span class="p">)</span><span class="w">
  
  </span><span class="k">while</span><span class="p">(</span><span class="nf">abs</span><span class="p">(</span><span class="n">fc</span><span class="p">)</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">epsilon</span><span class="p">){</span><span class="w">
    </span><span class="k">if</span><span class="p">(</span><span class="n">fx</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">beta</span><span class="o">=</span><span class="n">l</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">fc</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="m">0</span><span class="p">)</span><span class="w"> </span><span class="n">u</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cx</span><span class="w">
    </span><span class="k">if</span><span class="p">(</span><span class="n">fx</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">beta</span><span class="o">=</span><span class="n">l</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">fc</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="m">0</span><span class="p">)</span><span class="w"> </span><span class="n">l</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cx</span><span class="w">
    </span><span class="n">cx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">l</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">u</span><span class="p">)</span><span class="o">/</span><span class="m">2</span><span class="w">
    </span><span class="n">fc</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">fx</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">beta</span><span class="o">=</span><span class="n">cx</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">)</span><span class="w">
    
    </span><span class="n">c_rec</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="n">c_rec</span><span class="p">,</span><span class="w"> </span><span class="n">cx</span><span class="p">)</span><span class="w">
    </span><span class="n">fc_rec</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="n">fc_rec</span><span class="p">,</span><span class="w"> </span><span class="n">fc</span><span class="p">)</span><span class="w">
    </span><span class="n">iter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">iter</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">1</span><span class="w">
  </span><span class="p">}</span><span class="w">
  </span><span class="n">cat</span><span class="p">(</span><span class="n">paste0</span><span class="p">(</span><span class="s1">'Converged in '</span><span class="p">,</span><span class="w"> </span><span class="n">iter</span><span class="p">,</span><span class="s1">' iterations.\nRoot found at '</span><span class="p">,</span><span class="w"> </span><span class="n">cx</span><span class="p">,</span><span class="w"> </span><span class="s1">'.\n'</span><span class="p">))</span><span class="w">
  
  </span><span class="k">if</span><span class="p">(</span><span class="n">plotting</span><span class="o">==</span><span class="kc">FALSE</span><span class="p">)</span><span class="w"> </span><span class="nf">return</span><span class="p">(</span><span class="n">cx</span><span class="p">)</span><span class="w">
  
  </span><span class="n">range_vals</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">seq</span><span class="p">(</span><span class="nf">min</span><span class="p">(</span><span class="n">initials</span><span class="p">),</span><span class="w"> </span><span class="nf">max</span><span class="p">(</span><span class="n">initials</span><span class="p">),</span><span class="w"> </span><span class="m">0.001</span><span class="p">)</span><span class="w">
  </span><span class="n">rec</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">()</span><span class="w">
  </span><span class="k">for</span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">range_vals</span><span class="p">)</span><span class="w"> </span><span class="n">rec</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="n">rec</span><span class="p">,</span><span class="w"> </span><span class="n">fx</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">))</span><span class="w">
  </span><span class="n">plot</span><span class="p">(</span><span class="n">range_vals</span><span class="p">,</span><span class="w"> </span><span class="n">rec</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">'l'</span><span class="p">,</span><span class="w"> </span><span class="n">xlab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">'Range of values, beta'</span><span class="p">,</span><span class="w"> 
       </span><span class="n">ylab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">'f(beta): Gradient of log-likelihood'</span><span class="p">,</span><span class="w"> 
       </span><span class="n">main</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">paste0</span><span class="p">(</span><span class="s1">'Bisection Algorithm Steps (converged in '</span><span class="p">,</span><span class="w"> </span><span class="n">iter</span><span class="p">,</span><span class="s1">' iterations)'</span><span class="p">))</span><span class="w">
  </span><span class="n">lines</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="nf">min</span><span class="p">(</span><span class="n">initials</span><span class="p">),</span><span class="w"> </span><span class="nf">max</span><span class="p">(</span><span class="n">initials</span><span class="p">)),</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">0</span><span class="p">),</span><span class="w"> 
        </span><span class="n">type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">'l'</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">'red'</span><span class="p">,</span><span class="w"> </span><span class="n">lty</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">'dashed'</span><span class="p">,</span><span class="w"> </span><span class="n">lwd</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1.75</span><span class="p">)</span><span class="w">
  </span><span class="n">points</span><span class="p">(</span><span class="n">c_rec</span><span class="p">,</span><span class="w"> </span><span class="n">fc_rec</span><span class="p">,</span><span class="w"> </span><span class="n">pch</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">'x'</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">'navy'</span><span class="p">)</span><span class="w">
  </span><span class="nf">return</span><span class="p">(</span><span class="n">cx</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="c1"># Example below</span><span class="w">
</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sim_p1</span><span class="p">[[</span><span class="m">1</span><span class="p">]];</span><span class="w"> </span><span class="n">beta</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sim_p1</span><span class="p">[[</span><span class="m">2</span><span class="p">]];</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sim_p1</span><span class="p">[[</span><span class="m">3</span><span class="p">]]</span><span class="w">
</span><span class="n">root</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bisection_find</span><span class="p">(</span><span class="n">fx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">eval_grad_logLik</span><span class="p">,</span><span class="w"> </span><span class="n">epsilon</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.0001</span><span class="p">,</span><span class="w"> </span><span class="n">initials</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">-5</span><span class="p">,</span><span class="w"> </span><span class="m">5</span><span class="p">),</span><span class="w"> 
                      </span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">plotting</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="/assets/img/blogPosts/optim2.png" style="width:75%; display: block; margin: auto;">
<br></p>

<p>As shown above, our bisection algorithm converged in 22 iterations, finding a root at 0.99999952, which is very close to the true parameter value of \(\beta = 1\) in the 1-dimensional case.</p>

<p>We now turn to gradient-ascend algorithms to find the MLE of \(\boldsymbol\beta\) for the general \(p\)-dimensional case.</p>

<p><br></p>

<h3 id="gradient-based-optimization-methods">Gradient-based optimization methods</h3>


    </div>
  </article>


</div>

          </div>
          <!-- sidebar, which will move to the top on a small screen -->
          <div class="col-sm-1">
            <nav id="toc-sidebar" class="sticky-top" style="white-space: nowrap;"></nav>
          </div>
        </div>
        
      
    </div>

    <!-- Footer -->    <br>
    <footer class="sticky-bottom mt-5">
      <div class="container" style="text-align: center;">
        © Copyright 2024 Ahmad  Abdel-Azim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>
  <!-- Sidebar Table of Contents -->
  <script defer src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script>


  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
